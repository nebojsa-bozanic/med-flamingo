{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPgoDD8Eh1GrsKVaDOqaGwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nebojsa-bozanic/med-flamingo/blob/master/med_flamingo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [ ] check https://www.eventbrite.com/e/efficient-fine-tuning-for-llama-7b-on-a-single-gpu-tickets-696331224437"
      ],
      "metadata": {
        "id": "M8deBEJRYaSe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0Uof9i9mhsv",
        "outputId": "10fdd9ae-b099-4477-f5f6-c474472a3e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'med-flamingo'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 30 (delta 6), reused 26 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (30/30), 426.85 KiB | 22.47 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/snap-stanford/med-flamingo.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git lfs install\n",
        "# %cd /content/med-flamingo\n",
        "# !mkdir models\n",
        "# %cd /content/med-flamingo/models\n",
        "# !git clone https://huggingface.co/decapoda-research/llama-7b-hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqa0xDJKm3uQ",
        "outputId": "d4e21bda-d18e-431e-def4-ecef1857fa4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "/content/med-flamingo\n",
            "/content/med-flamingo/models\n",
            "Cloning into 'llama-7b-hf'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Unpacking objects: 100% (91/91), 22.58 KiB | 1.33 MiB/s, done.\n",
            "Filtering content: 100% (34/34), 12.55 GiB | 155.13 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install open_flamingo, accelerate, bitsandbytes, and transformers libraries\n",
        "!pip install open_flamingo accelerate bitsandbytes transformers -q"
      ],
      "metadata": {
        "id": "AVvp6zjHXruE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/med-flamingo/scripts\n",
        "!python demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrI2D_mem7A4",
        "outputId": "980df4ab-3dba-46dd-a5d4-b6e0bae1c523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/med-flamingo/scripts\n",
            "2023-08-17 21:56:03.696643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading model..\n",
            "/usr/local/lib/python3.10/dist-packages/open_clip/pretrained.py:309: UserWarning: /root/.cache/clip/ViT-L-14.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
            "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
            "100%|████████████████████████████████████████| 933M/933M [00:07<00:00, 130MiB/s]\n",
            "Using pad_token, but it is not set yet.\n",
            "Loading checkpoint shards: 100% 33/33 [00:14<00:00,  2.24it/s]\n",
            "Flamingo model initialized with 1309919248 trainable parameters\n",
            "Downloading model.pt: 100% 5.24G/5.24G [00:13<00:00, 401MB/s]\n",
            "Downloaded Med-Flamingo checkpoint to /root/.cache/huggingface/hub/models--med-flamingo--med-flamingo/snapshots/7243cd83bd426ceade9c4de9844cc5e5f3ff75e0/model.pt\n",
            "Preprocess data\n",
            "Generate from multimodal few-shot prompt\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1253: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "response=\"You are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image> Question: What is/are the structure near/in the middle of the brain? Answer: pons.<|endofchunk|><image> Question: Is there evidence of a right apical pneumothorax on this chest x-ray? Answer: yes.<|endofchunk|><image> Question: Is/Are there air in the patient's peritoneal cavity? Answer: no.<|endofchunk|><image> Question: Does the heart appear enlarged? Answer: yes.<|endofchunk|><image> Question: What side are the infarcts located? Answer: bilateral.<|endofchunk|><image> Question: Which image modality is this? Answer: mr flair.<|endofchunk|><image> Question: Where is the largest mass located in the cerebellum? Answer: in the right cerebellum.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaForCausalLM\n",
        "\n",
        "# Store the original forward method\n",
        "old_forward = LlamaForCausalLM.forward\n",
        "\n",
        "# Define a custom forward method for LlamaForCausalLM\n",
        "def forward(self, input_ids, attention_mask, **kwargs):\n",
        "    \"\"\"\n",
        "    Condition the Flamingo layers on the media locations before forward().\n",
        "    Overrides the forward method to customize behavior.\n",
        "    \"\"\"\n",
        "    if not self.initialized_flamingo:\n",
        "        raise ValueError(\n",
        "            \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n",
        "        )\n",
        "\n",
        "    # Identify media locations\n",
        "    media_locations = input_ids == self.media_token_id\n",
        "\n",
        "    # Check for cached media and media tokens in the input\n",
        "    use_cached_media_locations = (\n",
        "        self._use_cached_vision_x\n",
        "        and self.is_conditioned()\n",
        "        and not media_locations.any()\n",
        "    )\n",
        "\n",
        "    # Condition layers based on media locations\n",
        "    for layer in self._get_decoder_layers():\n",
        "        if not use_cached_media_locations:\n",
        "            layer.condition_media_locations(media_locations)\n",
        "        layer.condition_use_cached_media(use_cached_media_locations)\n",
        "\n",
        "    # Prepare arguments for the original forward method\n",
        "    kwargs[\"input_ids\"] = input_ids\n",
        "    kwargs[\"attention_mask\"] = attention_mask\n",
        "    return old_forward(self, **kwargs)  # Call the original forward method\n",
        "\n",
        "# Override LlamaForCausalLM's forward method with the custom method\n",
        "LlamaForCausalLM.forward = forward"
      ],
      "metadata": {
        "id": "kQurHcz9YKsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import open_clip\n",
        "\n",
        "from open_flamingo.src.flamingo import Flamingo\n",
        "from open_flamingo.src.flamingo_lm import FlamingoLMMixin\n",
        "from open_flamingo.src.utils import extend_instance\n",
        "\n",
        "def create_model_and_transforms(\n",
        "    clip_vision_encoder_path: str,\n",
        "    clip_vision_encoder_pretrained: str,\n",
        "    lang_encoder_path: str,\n",
        "    tokenizer_path: str,\n",
        "    cross_attn_every_n_layers: int = 1,\n",
        "    use_local_files: bool = False,\n",
        "    decoder_layers_attr_name: str = None,\n",
        "    freeze_lm_embeddings: bool = False,\n",
        "    **flamingo_kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    Initialize a Flamingo model from pretrained vision encoder and language encoder.\n",
        "    Args:\n",
        "        clip_vision_encoder_path (str): Path to pretrained clip model (e.g. \"ViT-B-32\").\n",
        "        clip_vision_encoder_pretrained (str): Name of pretraining dataset for clip model (e.g. \"laion2b_s32b_b79k\").\n",
        "        lang_encoder_path (str): Path to pretrained language encoder.\n",
        "        tokenizer_path (str): Path to pretrained tokenizer.\n",
        "        cross_attn_every_n_layers (int, optional): Determines how often to add a cross-attention layer. Defaults to 1.\n",
        "        use_local_files (bool, optional): Whether to use local files. Defaults to False.\n",
        "        decoder_layers_attr_name (str, optional): Name of the decoder layers attribute. Defaults to None.\n",
        "    Returns:\n",
        "        Flamingo: Flamingo model from pretrained vision and language encoders.\n",
        "        Image processor: Pipeline to preprocess input images.\n",
        "        Tokenizer: A tokenizer for the language model.\n",
        "    \"\"\"\n",
        "    # Load the pretrained vision encoder\n",
        "    vision_encoder, _, image_processor = open_clip.create_model_and_transforms(\n",
        "        clip_vision_encoder_path, pretrained=clip_vision_encoder_pretrained\n",
        "    )\n",
        "    vision_encoder.visual.output_tokens = True  # Set the vision encoder to output visual features\n",
        "\n",
        "    # Load the tokenizer\n",
        "    text_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        tokenizer_path,\n",
        "        local_files_only=use_local_files,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    # Add Flamingo special tokens to the tokenizer\n",
        "    text_tokenizer.add_special_tokens(\n",
        "        {\"additional_special_tokens\": [\"\", \"<image>\"]}\n",
        "    )\n",
        "    if text_tokenizer.pad_token is None:\n",
        "        # Issue: GPT models don't have a pad token, which we use to modify labels for the loss\n",
        "        text_tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
        "\n",
        "    # Load the pretrained language encoder\n",
        "    lang_encoder = AutoModelForCausalLM.from_pretrained(\n",
        "        lang_encoder_path,\n",
        "        local_files_only=use_local_files,\n",
        "        trust_remote_code=True,\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "\n",
        "    # Convert LM to FlamingoLM\n",
        "    extend_instance(lang_encoder, FlamingoLMMixin)\n",
        "\n",
        "    # Infer decoder layers attribute name\n",
        "    if decoder_layers_attr_name is None:\n",
        "        decoder_layers_attr_name = _infer_decoder_layers_attr_name(lang_encoder)\n",
        "    lang_encoder.set_decoder_layers_attr_name(decoder_layers_attr_name)\n",
        "    lang_encoder.resize_token_embeddings(len(text_tokenizer))\n",
        "\n",
        "    # Create Flamingo model\n",
        "    model = Flamingo(\n",
        "        vision_encoder,\n",
        "        lang_encoder,\n",
        "        text_tokenizer.encode(\"\")[-1],\n",
        "        text_tokenizer.encode(\"<image>\")[-1],\n",
        "        vis_dim=open_clip.get_model_config(clip_vision_encoder_path)[\"vision_cfg\"][\"width\"],\n",
        "        cross_attn_every_n_layers=cross_attn_every_n_layers,\n",
        "        **flamingo_kwargs,\n",
        "    )\n",
        "\n",
        "    # Freeze necessary parameters\n",
        "    model.requires_grad_(False)\n",
        "    assert sum(p.numel() for p in model.parameters() if p.requires_grad) == 0\n",
        "\n",
        "    # Unfreeze specified components\n",
        "    model.perceiver.requires_grad_(True)\n",
        "    model.lang_encoder.gated_cross_attn_layers.requires_grad_(True)\n",
        "    if not freeze_lm_embeddings:\n",
        "        model.lang_encoder.get_input_embeddings().requires_grad_(True)\n",
        "        # TODO: investigate also training the output embeddings when untied\n",
        "\n",
        "    # Display initialization information\n",
        "    print(\n",
        "        f\"Flamingo model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\"\n",
        "    )\n",
        "\n",
        "    return model, image_processor, text_tokenizer\n",
        "\n",
        "def _infer_decoder_layers_attr_name(model):\n",
        "    # Infer decoder layers attribute name based on model's class name\n",
        "    for k in __KNOWN_DECODER_LAYERS_ATTR_NAMES:\n",
        "        if k.lower() in model.__class__.__name__.lower():\n",
        "            return __KNOWN_DECODER_LAYERS_ATTR_NAMES[k]\n",
        "\n",
        "    # Raise error if attribute name cannot be inferred\n",
        "    raise ValueError(\n",
        "        f\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\n",
        "    )\n",
        "\n",
        "# Known decoder layers attribute names mapping\n",
        "__KNOWN_DECODER_LAYERS_ATTR_NAMES = {\n",
        "    \"opt\": \"model.decoder.layers\",\n",
        "    \"gptj\": \"transformer.h\",\n",
        "    \"gpt-j\": \"transformer.h\",\n",
        "    \"pythia\": \"gpt_neox.layers\",\n",
        "    \"llama\": \"model.layers\",\n",
        "    \"gptneoxforcausallm\": \"gpt_neox.layers\",\n",
        "    \"mpt\": \"transformer.blocks\",\n",
        "    \"mosaicgpt\": \"transformer.blocks\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "zaWGBzvhYMcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from huggingface_hub import hf_hub_download\n",
        "import torch\n",
        "\n",
        "# Download the Med-Flamingo checkpoint from the Hugging Face Hub\n",
        "checkpoint_path = hf_hub_download(\"med-flamingo/med-flamingo\", \"model.pt\")\n",
        "print(f'Downloaded Med-Flamingo checkpoint to {checkpoint_path}')\n",
        "\n",
        "# Load the downloaded checkpoint using PyTorch\n",
        "med_flamingo_checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")"
      ],
      "metadata": {
        "id": "DA89KRUoYN6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from huggingface_hub import hf_hub_download\n",
        "import torch\n",
        "import os\n",
        "from accelerate import Accelerator  # Import Accelerate library for hardware acceleration\n",
        "from einops import repeat\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "# Append paths for custom modules\n",
        "sys.path.append('/content/med-flamingo/scripts')\n",
        "sys.path.append('/content/med-flamingo')\n",
        "from src.utils import FlamingoProcessor\n",
        "from demo_utils import image_paths, clean_generation\n",
        "\n",
        "# Initialize Accelerator\n",
        "accelerator = Accelerator()  # Use hardware acceleration (GPU or TPU) based on availability\n",
        "device = accelerator.device\n",
        "\n",
        "print('Loading model..')\n",
        "\n",
        "# Import create_model_and_transforms function\n",
        "from open_flamingo import create_model_and_transforms\n",
        "\n",
        "# Initialize the Flamingo model, image processor, and tokenizer\n",
        "model, image_processor, tokenizer = create_model_and_transforms(\n",
        "    clip_vision_encoder_path=\"ViT-L-14\",\n",
        "    clip_vision_encoder_pretrained=\"openai\",\n",
        "    lang_encoder_path=\"huggyllama/llama-7b\",\n",
        "    tokenizer_path=\"huggyllama/llama-7b\",\n",
        "    cross_attn_every_n_layers=4,\n",
        ")"
      ],
      "metadata": {
        "id": "caDYuvGRYPFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory cache to free up memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Move the perceiver and vision_encoder of the model to GPU\n",
        "model.perceiver.cuda()\n",
        "model.vision_encoder.cuda()\n",
        "\n",
        "# Convert the gated_cross_attn_layers of the lang_encoder to float16 and move to GPU\n",
        "model.lang_encoder.gated_cross_attn_layers.to(torch.float16).cuda()\n",
        "\n",
        "# Clear GPU memory cache again after moving tensors to GPU\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "zIs5B9vIYQp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc  # Import the garbage collection module\n",
        "\n",
        "# Collect and free up unused memory\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "MRfD8xpzYRsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Med-Flamingo checkpoint into the model (not enforcing strict compatibility)\n",
        "model.load_state_dict(med_flamingo_checkpoint, strict=False)\n",
        "\n",
        "# Initialize the FlamingoProcessor using the tokenizer and image_processor\n",
        "processor = FlamingoProcessor(tokenizer, image_processor)\n"
      ],
      "metadata": {
        "id": "-JU2NE0RYSvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd med-flamingo/src"
      ],
      "metadata": {
        "id": "HJJwsyDBYUfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the first 2 image paths\n",
        "image_paths = image_paths[:2]\n",
        "\n",
        "# Prepare the model using the Accelerator\n",
        "model = accelerator.prepare(model)\n",
        "is_main_process = accelerator.is_main_process\n",
        "model.eval()\n",
        "\n",
        "\"\"\"\n",
        "Step 1: Load images\n",
        "\"\"\"\n",
        "# Load demo images using PIL and store in a list\n",
        "demo_images = [Image.open(path) for path in image_paths]\n",
        "\n",
        "\"\"\"\n",
        "Step 2: Define multimodal few-shot prompt\n",
        "\"\"\"\n",
        "\n",
        "# Define a few-shot prompt containing text and <image> placeholders\n",
        "prompt = \"You are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image>Question: What is/are the structure near/in the middle of the brain? Answer: pons.<image>Question: Is there evidence of a right apical pneumothorax on this chest x-ray? Answer: yes.<image>Question: Is/Are there air in the patient's peritoneal cavity? Answer: no.<image>Question: Does the heart appear enlarged? Answer: yes.<image>Question: What side are the infarcts located? Answer: bilateral.<image>Question: Which image modality is this? Answer: mr flair.<image>Question: What is the most likely diagnosis?\"\n",
        "\n",
        "\"\"\"\n",
        "Step 3: Preprocess data\n",
        "\"\"\"\n",
        "print('Preprocess data')\n",
        "\n",
        "# Preprocess demo images using the FlamingoProcessor\n",
        "pixels = processor.preprocess_images(demo_images)\n",
        "\n",
        "pixels = repeat(pixels, 'N c h w -> b N T c h w', b=1, T=1)\n",
        "\n",
        "# Encode the text prompt using the FlamingoProcessor\n",
        "tokenized_data = processor.encode_text(prompt)\n"
      ],
      "metadata": {
        "id": "_Yl7dPrzYWUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 4: Generate response\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the multimodal few-shot prompt\n",
        "print('Generate from multimodal few-shot prompt')\n",
        "\n",
        "# Use mixed-precision training context for improved performance\n",
        "with torch.autocast('cuda', torch.float16):\n",
        "\n",
        "    # Generate text using the model\n",
        "    generated_text = model.generate(\n",
        "        vision_x=pixels.to(device),  # Convert images to the device\n",
        "        lang_x=tokenized_data[\"input_ids\"].to(device),  # Convert text input to the device\n",
        "        attention_mask=tokenized_data[\"attention_mask\"].to(device),  # Convert attention mask to the device\n",
        "        max_new_tokens=20,  # Limit the maximum number of new tokens in the generated response\n",
        "    )\n",
        "\n",
        "# Decode the generated text using the processor's tokenizer\n",
        "response = processor.tokenizer.decode(generated_text[0])\n",
        "\n",
        "# Clean up the generated response\n",
        "response = clean_generation(response)\n",
        "\n",
        "# Print the cleaned response\n",
        "print(f'{response=}')\n"
      ],
      "metadata": {
        "id": "c5VegOVCYYBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated response\n",
        "response"
      ],
      "metadata": {
        "id": "DpgaoLNpYZUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}